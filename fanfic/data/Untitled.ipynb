{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24f714e-bc7b-4bd7-a926-1b6cd2276670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re, sys\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1ec06a-0fc4-406b-8d57-0b644f30ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-o OUT]\n",
      "                             [--sort {best,top,new,controversial,old,q&a}]\n",
      "                             [--weight-by-score] [--no-chart]\n",
      "                             url\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/river_one/.pyenv/versions/3.12.7/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# reddit_ao3_ai_sentiment.py\n",
    "import argparse, json, re, sys, time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "UA = \"AO3-Reddit-Sentiment/2.0 (contact: you@example.com)\"\n",
    "\n",
    "# ---------- Sentiment (VADER) ----------\n",
    "def get_sia():\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    import nltk\n",
    "    try:\n",
    "        nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"vader_lexicon\", quiet=True)\n",
    "    return SentimentIntensityAnalyzer()\n",
    "\n",
    "# ---------- URL helpers ----------\n",
    "def ensure_scheme(url: str) -> str:\n",
    "    \"\"\"Add https:// if missing.\"\"\"\n",
    "    if not url.lower().startswith((\"http://\", \"https://\")):\n",
    "        return \"https://\" + url.lstrip(\"/\")\n",
    "    return url\n",
    "\n",
    "def to_json_url(thread_url: str, sort: str = \"best\") -> str:\n",
    "    \"\"\"\n",
    "    Normalize a Reddit thread URL to its JSON endpoint.\n",
    "    Works whether or not you include the trailing slash or .json.\n",
    "    \"\"\"\n",
    "    url = ensure_scheme(thread_url.strip())\n",
    "    if not url.endswith(\"/\"):\n",
    "        url += \"/\"\n",
    "    if url.endswith(\".json/\"):\n",
    "        return url + f\"?sort={sort}\"\n",
    "    if url.endswith(\".json/\") or url.endswith(\".json\"):\n",
    "        return url + f\"?sort={sort}\"\n",
    "    return url + f\".json?sort={sort}\"\n",
    "\n",
    "# ---------- HTTP fetch with retries ----------\n",
    "def fetch_json(url: str, max_retries: int = 4, base_sleep: float = 1.5):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers={\"User-Agent\": UA}, timeout=30)\n",
    "            if r.status_code == 429:\n",
    "                retry_after = r.headers.get(\"Retry-After\")\n",
    "                wait = float(retry_after) if retry_after else base_sleep * attempt\n",
    "                print(f\"Rate limited (429). Sleeping {wait:.1f}s...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            wait = base_sleep * attempt\n",
    "            print(f\"Fetch error ({e}). Retry {attempt}/{max_retries} in {wait:.1f}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "# ---------- Flatten reddit comment tree ----------\n",
    "def walk_comments(node, out, link_id, parent_id=None, depth=0,\n",
    "                  skip_authors={\"[deleted]\", \"AutoModerator\"}):\n",
    "    data = node.get(\"data\", {})\n",
    "    if \"body\" in data:\n",
    "        author = data.get(\"author\")\n",
    "        if author not in skip_authors:\n",
    "            body = data.get(\"body\") or \"\"\n",
    "            # Clean: remove blockquotes and collapse whitespace\n",
    "            clean = re.sub(r\"(?m)^>.*\\n?\", \"\", body)\n",
    "            clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "            out.append({\n",
    "                \"link_id\": link_id,\n",
    "                \"comment_id\": data.get(\"id\"),\n",
    "                \"parent_id\": data.get(\"parent_id\"),\n",
    "                \"depth\": depth,\n",
    "                \"author\": author,\n",
    "                \"score\": data.get(\"score\"),\n",
    "                \"created_utc\": data.get(\"created_utc\"),\n",
    "                \"created_iso\": datetime.utcfromtimestamp(\n",
    "                    data.get(\"created_utc\")\n",
    "                ).isoformat() if data.get(\"created_utc\") else \"\",\n",
    "                \"body\": clean\n",
    "            })\n",
    "\n",
    "    # Recurse into replies\n",
    "    replies = data.get(\"replies\")\n",
    "    if isinstance(replies, dict):\n",
    "        children = replies.get(\"data\", {}).get(\"children\", [])\n",
    "        for c in children:\n",
    "            if c.get(\"kind\") == \"t1\":  # <- keep the colon!\n",
    "                walk_comments(c, out, link_id, data.get(\"id\"), depth + 1, skip_authors)\n",
    "\n",
    "def load_comments(json_root) -> pd.DataFrame:\n",
    "    # Reddit post is j[0], comments tree is j[1]\n",
    "    post = json_root[0][\"data\"][\"children\"][0][\"data\"]\n",
    "    link_id = post.get(\"id\")\n",
    "    children = json_root[1][\"data\"][\"children\"]\n",
    "\n",
    "    out = []\n",
    "    for c in children:\n",
    "        if c.get(\"kind\") == \"t1\":  # <- keep the colon!\n",
    "            walk_comments(c, out, link_id)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# ---------- Sentiment analysis ----------\n",
    "def analyze_sentiment(df: pd.DataFrame, weight_by_score: bool = False):\n",
    "    from numpy import average\n",
    "    sia = get_sia()\n",
    "    scores = df[\"body\"].apply(lambda t: sia.polarity_scores(t))\n",
    "    sent = pd.DataFrame(list(scores))\n",
    "    df = pd.concat([df, sent], axis=1)\n",
    "\n",
    "    def bucket(x):\n",
    "        if x >= 0.05: return \"positive\"\n",
    "        if x <= -0.05: return \"negative\"\n",
    "        return \"neutral\"\n",
    "    df[\"sentiment\"] = df[\"compound\"].apply(bucket)\n",
    "\n",
    "    if weight_by_score:\n",
    "        weights = (df[\"score\"].fillna(0) + 1).clip(lower=0)\n",
    "        avg_compound = float(average(df[\"compound\"], weights=weights))\n",
    "    else:\n",
    "        avg_compound = float(df[\"compound\"].mean())\n",
    "\n",
    "    summary = {\n",
    "        \"n_comments\": int(len(df)),\n",
    "        \"avg_compound\": avg_compound,\n",
    "        \"share_negative\": float((df[\"sentiment\"]==\"negative\").mean()),\n",
    "        \"share_neutral\":  float((df[\"sentiment\"]==\"neutral\").mean()),\n",
    "        \"share_positive\": float((df[\"sentiment\"]==\"positive\").mean())\n",
    "    }\n",
    "    return df, summary\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Scrape a Reddit thread's comments and run sentiment analysis.\"\n",
    "    )\n",
    "    p.add_argument(\"url\", help=\"Full Reddit thread URL (include https://)\")\n",
    "    p.add_argument(\"-o\", \"--out\", default=None, help=\"Output CSV filename (default auto)\")\n",
    "    p.add_argument(\"--sort\", choices=[\"best\",\"top\",\"new\",\"controversial\",\"old\",\"q&a\"],\n",
    "                   default=\"best\", help=\"Sort for the Reddit JSON endpoint\")\n",
    "    p.add_argument(\"--weight-by-score\", action=\"store_true\",\n",
    "                   help=\"Weight average sentiment by comment score\")\n",
    "    p.add_argument(\"--no-chart\", action=\"store_true\",\n",
    "                   help=\"Skip saving sentiment bar chart\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Guard against flags being passed as the \"url\"\n",
    "    if args.url.startswith(\"-\"):\n",
    "        print(\"Error: First positional argument must be the Reddit thread URL (including https://).\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    jurl = to_json_url(args.url, sort=args.sort)\n",
    "    print(f\"Fetching JSON: {jurl}\")\n",
    "    try:\n",
    "        j = fetch_json(jurl)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch JSON: {e}\")\n",
    "        sys.exit(2)\n",
    "\n",
    "    try:\n",
    "        df = load_comments(j)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse comments: {e}\")\n",
    "        sys.exit(3)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No comments found (thread may be empty, locked, or requires login).\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    df, summary = analyze_sentiment(df, weight_by_score=args.weight_by_score)\n",
    "\n",
    "    # Save CSV\n",
    "    if args.out:\n",
    "        out_csv = args.out\n",
    "    else:\n",
    "        host = urlparse(args.url).netloc.replace(\".\", \"_\")\n",
    "        out_csv = f\"reddit_comments_{host}.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(df)} comments to {out_csv}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Sentiment Summary ---\")\n",
    "    for k, v in summary.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Examples\n",
    "    print(\"\\nMost negative examples:\")\n",
    "    print(df.sort_values(\"compound\").head(5)[[\"compound\",\"body\"]].to_string(index=False, max_colwidth=120))\n",
    "    print(\"\\nMost positive examples:\")\n",
    "    print(df.sort_values(\"compound\", ascending=False).head(5)[[\"compound\",\"body\"]].to_string(index=False, max_colwidth=120))\n",
    "\n",
    "    # Optional chart\n",
    "    if not args.no_chart:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            counts = df[\"sentiment\"].value_counts().reindex([\"negative\",\"neutral\",\"positive\"]).fillna(0)\n",
    "            fig, ax = plt.subplots(figsize=(5,4))\n",
    "            ax.bar(counts.index, counts.values)\n",
    "            ax.set_title(\"Reddit Sentiment on AI-Generated Writing\")\n",
    "            ax.set_ylabel(\"Number of comments\")\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(\"reddit_ao3_ai_sentiment_counts.png\", dpi=200)\n",
    "            print(\"Saved bar chart: reddit_ao3_ai_sentiment_counts.png\")\n",
    "        except Exception as e:\n",
    "            print(\"Chart skipped:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f277b4-e662-43f5-8049-454fc7c1f645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
